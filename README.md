# NanoChatâ€‘X ðŸ§ ðŸ’¬

A **minimal transformer chatbot**, inspired by Andrej Karpathyâ€™s teaching style.  
NanoChatâ€‘X is designed to help you understand every moving part of a GPTâ€‘like model by building the lightest possible version, then gradually extending it.

---

## ðŸ“‚ Project Structure

```
NanoChat-X/
 â”œâ”€â”€ data/
 â”‚    â”œâ”€â”€ cornell_movie_dialogs/   # raw dataset files
 â”‚    â””â”€â”€ data.txt                 # preprocessed Q->A pairs
 â”œâ”€â”€ src/
 â”‚    â”œâ”€â”€ model.py                 # NanoTransformer definition
 â”‚    â”œâ”€â”€ tokenizer.py             # encode/decode functions
 â”‚    â”œâ”€â”€ train.py                 # training loop
 â”‚    â”œâ”€â”€ chat.py                  # interactive chat loop
 â”‚    â””â”€â”€ __init__.py
 â”œâ”€â”€ utils/
 â”‚    â””â”€â”€ helpers.py               # training logs, generation helpers
 â”œâ”€â”€ requirements.txt              # dependencies
 â””â”€â”€ Readme.md                     # project documentation
```

---

## ðŸš€ Getting Started

### 1. Setup
Install Python (â‰¥3.9 recommended) and PyTorch:

```bash
pip install torch
```

Install other dependencies:

```bash
pip install -r requirements.txt
```

### 2. Prepare Data
Start with a tiny toy dataset in `data/data.txt`:

```
Hello -> Hi
How are you? -> I am fine
What is your name? -> I am Nanochat
Bye -> Goodbye
```

Later, you can preprocess the Cornell Movie Dialogs corpus into Qâ†’A pairs and place it in `data/data.txt`.

### 3. Train
Run:

```bash
python src/train.py
```

This trains the `NanoTransformer` on your dataset and saves a checkpoint (`nanochat_model.pt`).

### 4. Chat
Run:

```bash
python src/chat.py
```

Youâ€™ll enter an interactive loop:

```
You: Hello
Nanochat: Hi
```

---

## ðŸ§  How It Works

- **Tokenizer**: Converts text into tokens (characterâ€‘level or wordâ€‘level).  
- **NanoTransformer**: A tiny Transformer encoder with embeddings, positional encodings, and a linear head.  
- **Training Loop**: Learns nextâ€‘token prediction using crossâ€‘entropy loss.  
- **Chat Loop**: Generates responses autoregressively, sampling one token at a time.

---

## ðŸŒ± Extensions

Once youâ€™ve run the minimal version, you can extend NanoChatâ€‘X by:
- Switching to **wordâ€‘level or BPE tokenization**.  
- Using the **Cornell Movie Dialogs dataset** for richer conversations.  
- Adding **causal masking** for proper autoregressive generation.  
- Increasing model size (more layers, heads, embeddings).  
- Adding `<SOS>` and `<EOS>` tokens for cleaner start/stop behavior.  
- Experimenting with **temperature** and **topâ€‘k sampling** for more natural outputs.

---

## ðŸŽ¯ Goal

NanoChatâ€‘X is not meant to be a production chatbot.  
Itâ€™s a **learning project**: a handsâ€‘on way to understand how GPTâ€‘like models are built from scratch.

---
